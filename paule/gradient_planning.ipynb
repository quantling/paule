{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "import paule\n",
    "from paule.util import (speak, normalize_cp, inv_normalize_cp, normalize_mel_librosa, inv_normalize_mel_librosa, stereo_to_mono, librosa_melspec, pad_same_to_even_seq_length, RMSELoss, mel_to_sig, pad_batch_online)\n",
    "from models import *\n",
    "from matplotlib import cm\n",
    "\n",
    "DIR = os.getcwd()\n",
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/Volumes/T7/Common_Voice/data/\"\n",
    "RESULT_DICT = \"/Volumes/T7/Common_Voice/common_voice_test_planning\"\n",
    "SAVE_DICT = \"/Volumes/T7/Common_Voice/common_voice_test_planning/results_acoustic_lr_001_lr_model_00001_10_outer_50_inner_6_batches_2_epochs_only_new\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Paule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model = paule.Paule(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(DATA_DIR + \"common_voice_geco_words_test_subset_slim_prot4.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Flac Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i,row in test_data.iterrows():\n",
    "    sr = 48000\n",
    "    sig = row.wav_rec\n",
    "    file_name = row.file_name\n",
    "    label = row.label\n",
    "    \n",
    "    path = RESULT_DICT + label + \"/\"\n",
    "    if os.path.isdir(path):\n",
    "        file = path +  file_name + \"_\" + label + \".flac\"\n",
    "        if file.is_file():\n",
    "            j = 1\n",
    "            while file.is_file(): \n",
    "                file = path +  file_name + \"_\" + label + \"%d.flac\" % j\n",
    "                j+=1\n",
    "        sf.write(path +  file_name + \"_\" + label + \"3.flac\" ,sig, sr)\n",
    "    else:\n",
    "        os.mkdir(path)\n",
    "        sf.write(path +  file_name + \"_\" + label + \".flac\" ,sig, sr)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Shuffle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for dic in os.listdir(RESULT_DICT):\n",
    "    if (\".DS_Store\" in dic) or (\"results\" in dic) : # .DS_Store stores custom attributes of its containing folder in macOS\n",
    "        continue\n",
    "    else:\n",
    "        if not os.path.isdir(os.path.join(SAVE_DICT, dic)):\n",
    "            os.mkdir(os.path.join(SAVE_DICT, dic))\n",
    "        path = os.path.join(RESULT_DICT, dic)\n",
    "        for file in os.listdir(path):\n",
    "            if (\".DS_Store\" in file) or (\"._\" in file):\n",
    "                continue\n",
    "            else:\n",
    "                file = os.path.join(path,file)\n",
    "                files.append(file)\n",
    "random.seed(30112021)\n",
    "random.shuffle(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omit already planned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_planned_files = []\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \"planned\" in file: \n",
    "                    planned_file = \"_\".join(file.split(\"_\")[:-1]) + \".flac\" \n",
    "                    already_planned_files.append(planned_file)\n",
    "\n",
    "unplanned_files = [file for file in files if file.split(\"/\")[-1] not in already_planned_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load predictive model already used and further learned during planning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "pred_model = torch.load(f\"{save_file}_pred_model.pt\", map_location=DEVICE)\n",
    "optimizer = torch.load(f\"{save_file}_pred_optimizer.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paule_model.pred_model = model\n",
    "paule_model.pred_optimizer = optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Planning and store results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, file in enumerate(unplanned_files):\n",
    "    i += 0\n",
    "    target_acoustic = file #path + \"/\" + file\n",
    "    save_file = f\"{SAVE_DICT}/{os.path.dirname(file).split('/')[-1]}/{os.path.basename(file)[:-5]}\"\n",
    "    \n",
    "    results = paule_model.plan_resynth(learning_rate_planning=0.01,\n",
    "            learning_rate_learning=0.001,\n",
    "            target_acoustic=target_acoustic,\n",
    "            initialize_from=\"acoustic\",\n",
    "            objective=\"acoustic\",\n",
    "            n_outer=20, n_inner=50,\n",
    "            continue_learning=True,\n",
    "            add_training_data=False,\n",
    "            log_ii=1,\n",
    "            log_semantics=True,\n",
    "            n_batches=6, batch_size=8, n_epochs=5,\n",
    "            log_gradients=False,\n",
    "            plot=True, plot_save_file=save_file, seed=None,\n",
    "            verbose=True)\n",
    "    \n",
    "    # save model and optimizer\n",
    "    torch.save(paule_model.pred_model, f\"{save_file}_pred_model.pt\")\n",
    "    torch.save(paule_model.pred_optimizer, f\"{save_file}_pred_optimizer.pt\")\n",
    "\n",
    "    # save results without model and optimizer\n",
    "    with open(f\"{save_file}.pkl\", 'wb') as pfile:\n",
    "        pickle.dump(results, pfile)\n",
    "\n",
    "\n",
    "    # save initial and planned flac\n",
    "    prod_sr = results.prod_sr\n",
    "    sig_initial = results.sig_steps[0]\n",
    "    sf.write(save_file + \"_initial.flac\", sig_initial, prod_sr)\n",
    "    prod_sig = results.prod_sig\n",
    "    sf.write(save_file + \"_planned.flac\", prod_sig, prod_sr)\n",
    "\n",
    "    # save svgs\n",
    "    planned_cp = results.planned_cp\n",
    "    path = save_file + '_svgs/'\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    util.export_svgs(util.inv_normalize_cp(planned_cp), path=path)\n",
    "\n",
    "    # ffmpeg -r 80 -width 600 -i tract%05d.svg -i planned_0.flac planned_0.mp4\n",
    "    # /usr/bin/ffmpeg -r 80 -width 600 -i /home/tino/Documents/phd/projects/paule/results/000003-Wissenschaft_svgs/tract%05d.svg -i results/000003-Wissenschaft_planned.flac planned.mp4\n",
    "\n",
    "\n",
    "    # save loss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.planned_loss_steps, label=\"planned loss\", c=\"C0\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss.png\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.prod_loss_steps, label=\"produced mel loss\", c=\"C1\")\n",
    "    ax.plot(results.planned_mel_loss_steps, label=\"planned mel loss\", c=\"C0\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_mel.png\")\n",
    "\n",
    "    # save subloss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.vel_loss_steps, label=\"vel loss\", c=\"C2\")\n",
    "    ax.plot(results.jerk_loss_steps, label=\"jerk loss\", c=\"C3\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_subloss.png\")\n",
    "\n",
    "    # save semvec loss plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 8), facecolor=\"white\")\n",
    "    ax.plot(results.pred_semvec_loss_steps, label=\"planned semvec loss\", c=\"C0\")\n",
    "    ax.plot(results.prod_semvec_loss_steps, label=\"produced semvec loss\", c=\"C1\")\n",
    "    ax.legend()\n",
    "    fig.savefig(f\"{save_file}_loss_semvec.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model Loss for continued learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = []\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \".pkl\" in file: \n",
    "                    with open(os.path.join(path,file), 'rb') as f:\n",
    "                        results = pickle.load(f)\n",
    "                        model_loss += list((np.asarray(results[21][:-1:2]) + np.asarray(results[21][1::2]))/2)#results[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,8), facecolor = \"white\")\n",
    "ax.plot(model_loss, label = \"model loss\", c = \"C0\")\n",
    "#ax.plot(range(len(model_loss)),np.repeat(np.mean(model_loss),len(model_loss)) ,c = \"C1\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling with Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = pd.DataFrame(columns=['file_name','label', 'planned_cp', 'inv_cp', 'target_sig','inv_sig', 'prod_sig', 'target_mel', \n",
    "                                      'inv_mel','prod_mel','pred_mel', 'prod_mel_loss', 'pred_mel_loss', 'vel_loss', 'jerk_loss',\n",
    "                                     'prod_semvec','pred_semvec','prod_semvec_loss', 'pred_semvec_loss'])\n",
    "ix = 0\n",
    "\n",
    "for dic in os.listdir(SAVE_DICT):\n",
    "    if \".DS_Store\" in dic:\n",
    "        continue\n",
    "    else:\n",
    "        path = os.path.join(SAVE_DICT,dic)\n",
    "        if os.path.isdir(path):\n",
    "            for file in os.listdir(path):\n",
    "                if \".pkl\" in file: \n",
    "                    with open(os.path.join(path,file), 'rb') as f:\n",
    "                        results = pickle.load(f)\n",
    "                        \n",
    "                        file_name = \"_\".join(file.split(\"_\")[:-1])\n",
    "                        label=file.split(\"_\")[-1][:-4]\n",
    "                        planned_cp = results[0]\n",
    "                        inv_cp = results[1]\n",
    "                        target_sig = results[2]\n",
    "                        prod_sig = results[4]\n",
    "                        target_mel = results[3]\n",
    "                        \n",
    "                        inv_sig, sr = speak(inv_normalize_cp(inv_cp))\n",
    "                        inv_mel = librosa_melspec(inv_sig, sr)\n",
    "                        inv_mel = normalize_mel_librosa(inv_mel)\n",
    "                        prod_mel = results[5]\n",
    "                        pred_mel = results[6]\n",
    "                        prod_mel_loss = results[7][-1]\n",
    "                        pred_mel_loss = results[9][-1]\n",
    "                        vel_loss = results[10][-1]\n",
    "                        jerk_loss = results[11][-1]\n",
    "                        prod_semvec = results[16][-1]\n",
    "                        pred_semvec = results[15][-1]\n",
    "                        prod_semvec_loss = results[13][-1]\n",
    "                        pred_semvec_loss = results[12][-1]\n",
    "                        \n",
    "                        \n",
    "                        final_results.loc[ix] = [file_name,label,planned_cp,inv_cp,target_sig,inv_sig,prod_sig,target_mel,inv_mel,prod_mel,\n",
    "                                                pred_mel,prod_mel_loss,pred_mel_loss,vel_loss,jerk_loss,prod_semvec,\n",
    "                                                pred_semvec,prod_semvec_loss,pred_semvec_loss]\n",
    "                        ix+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_results.to_pickle(SAVE_DICT+\"/final_results.pkl\", protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
